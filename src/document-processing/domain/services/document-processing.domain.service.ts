import {
  Injectable,
  Logger,
  NotFoundException,
  ForbiddenException,
  BadRequestException,
  Inject,
} from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Cron, CronExpression } from '@nestjs/schedule';
import { DocumentRepositoryPort } from '../ports/document.repository.port';
import { StorageServicePort } from '../ports/storage.service.port';
import { OcrServicePort } from '../ports/ocr.service.port';
import { Document } from '../entities/document.entity';
import { ExtractedField } from '../entities/extracted-field.entity';
import { DocumentStatus } from '../enums/document-status.enum';
import { DocumentType } from '../enums/document-type.enum';
import { ProcessingMethod } from '../enums/processing-method.enum';
import { AllConfigType } from '../../../config/config.type';
import { AuditService } from '../../../audit/audit.service';
import { extractEntitiesFromText } from '../../utils/text-entity-extractor';
import { Pdf2JsonService } from '../../infrastructure/pdf-extraction/pdf2json.service';
import { DocumentStateMachine } from '../utils/document-state-machine.util';
import { Actor } from '../../../access-control/domain/services/access-grant.domain.service';
import { UserManagerAssignmentService } from '../../../users/domain/services/user-manager-assignment.service';

// Use require for pdf-parse (CommonJS module)
// pdf-parse exports { PDFParse } as a named export
// eslint-disable-next-line @typescript-eslint/no-require-imports
const { PDFParse: pdfParse } = require('pdf-parse');

export enum DocumentEventType {
  DOCUMENT_UPLOADED = 'DOCUMENT_UPLOADED',
  DOCUMENT_PROCESSING_STARTED = 'DOCUMENT_PROCESSING_STARTED',
  DOCUMENT_PROCESSING_COMPLETED = 'DOCUMENT_PROCESSING_COMPLETED',
  DOCUMENT_PROCESSING_FAILED = 'DOCUMENT_PROCESSING_FAILED',
  DOCUMENT_ACCESSED = 'DOCUMENT_ACCESSED',
  DOCUMENT_DELETED = 'DOCUMENT_DELETED',
  DOCUMENT_HARD_DELETED = 'DOCUMENT_HARD_DELETED',
  UNAUTHORIZED_DOCUMENT_ACCESS = 'UNAUTHORIZED_DOCUMENT_ACCESS',
}

/**
 * Domain Service for Document Processing
 *
 * Business logic for document upload, OCR processing, and lifecycle management.
 * All PHI handling must comply with HIPAA requirements.
 */
@Injectable()
export class DocumentProcessingDomainService {
  private readonly logger = new Logger(DocumentProcessingDomainService.name);
  private readonly retentionYears: number;
  private readonly maxRetryCount = 3;

  constructor(
    @Inject('DocumentRepositoryPort')
    private readonly documentRepository: DocumentRepositoryPort,
    @Inject('StorageServicePort')
    private readonly storageService: StorageServicePort,
    @Inject('OcrServicePort')
    private readonly ocrService: OcrServicePort,
    private readonly auditService: AuditService,
    private readonly configService: ConfigService<AllConfigType>,
    private readonly pdf2JsonService: Pdf2JsonService,
    private readonly userManagerAssignmentService: UserManagerAssignmentService,
  ) {
    this.retentionYears = this.configService.getOrThrow(
      'documentProcessing.retentionYears',
      { infer: true },
    );
  }

  /**
   * Upload document and initiate processing
   * 
   * Origin Manager Assignment:
   * - If actor is a manager → they become the origin manager
   * - If actor is a user → get their assigned manager (must have one assigned)
   */
  async uploadDocument(
    actor: Actor,
    fileBuffer: Buffer,
    fileName: string,
    mimeType: string,
    documentType: DocumentType,
    description?: string,
  ): Promise<Document> {
    try {
      // 1. Determine originManagerId
      let originManagerId: number;
      let originUserContextId: number | undefined;

      if (actor.type === 'manager') {
        // Manager uploads → they become the origin manager
        originManagerId = actor.id;
      } else if (actor.type === 'user') {
        // User uploads → get their assigned manager
        const assignedManagerIds =
          await this.userManagerAssignmentService.getAssignedManagerIds(
            actor.id,
          );

        if (assignedManagerIds.length === 0) {
          throw new BadRequestException(
            'User must have an assigned manager to upload documents. Please contact an administrator to assign a manager.',
          );
        }

        // Use the first assigned manager as origin manager
        // TODO: In future, allow user to select which manager if multiple assigned
        originManagerId = assignedManagerIds[0];
        originUserContextId = actor.id; // Track who uploaded (intake context)
      } else {
        throw new BadRequestException(
          'Only users and managers can upload documents',
        );
      }

      // 2. Create domain entity (initial state)
      const document = new Document();
      // NOTE: ID will be auto-generated by database as UUID
      document.userId = actor.id; // Uploader ID (for backward compatibility)
      document.originManagerId = originManagerId; // IMMUTABLE - set at creation
      document.originUserContextId = originUserContextId; // Optional: user who uploaded
      document.documentType = documentType;
      document.status = DocumentStatus.UPLOADED;
      document.fileName = fileName;
      document.fileSize = fileBuffer.length;
      document.mimeType = mimeType;
      document.description = description;
      document.rawFileUri = ''; // Will be set after upload
      document.uploadedAt = new Date();
      document.createdAt = new Date();
      document.updatedAt = new Date();
      document.retryCount = 0;

      // Set retention policy: scheduledDeletionAt = now + retentionYears
      const scheduledDeletionAt = new Date();
      scheduledDeletionAt.setFullYear(
        scheduledDeletionAt.getFullYear() + this.retentionYears,
      );
      document.scheduledDeletionAt = scheduledDeletionAt;

      // 2. Save to database (get UUID from database)
      const savedDocument = await this.documentRepository.save(document);

      // 3. Upload to GCS
      const gcsUri = await this.storageService.storeRaw(fileBuffer, {
        documentId: savedDocument.id,
        userId: actor.id,
        fileName,
        mimeType,
        contentLength: fileBuffer.length,
      });

      // 4. Update document with GCS URI and status (validate state transition)
      DocumentStateMachine.validateTransition(
        savedDocument.status,
        DocumentStatus.STORED,
      );
      await this.documentRepository.updateStatus(
        savedDocument.id,
        DocumentStatus.STORED,
        {
          rawFileUri: gcsUri,
        },
      );

      // 5. Audit log
      this.auditService.logAuthEvent({
        userId: String(actor.id),
        provider: 'document-processing',
        event: DocumentEventType.DOCUMENT_UPLOADED as any,
        success: true,
        metadata: {
          documentId: savedDocument.id,
          documentType,
          fileSize: fileBuffer.length,
          originManagerId,
          actorType: actor.type,
        },
      });

      this.logger.log(
        `Document uploaded: ${savedDocument.id} (actor: ${actor.type}:${actor.id}, originManager: ${originManagerId})`,
      );

      // 6. Trigger async processing (don't await) - pass buffer for PDF analysis
      this.startProcessing(
        savedDocument.id,
        gcsUri,
        mimeType,
        fileBuffer,
      ).catch((error) => {
        this.logger.error(
          `Failed to start processing for document ${savedDocument.id}: ${error.message}`,
        );
      });

      return savedDocument;
    } catch (error) {
      this.logger.error(`Upload failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Start OCR processing (async) - now with intelligent PDF routing
   */
  private async startProcessing(
    documentId: string,
    gcsUri: string,
    mimeType: string,
    fileBuffer?: Buffer,
  ): Promise<void> {
    try {
      // Get document to check current status
      const document = await this.documentRepository.findById(documentId);
      if (!document) throw new Error('Document not found');

      // Validate state transition before updating
      DocumentStateMachine.validateTransition(
        document.status,
        DocumentStatus.PROCESSING,
      );

      // Update status to PROCESSING
      await this.documentRepository.updateStatus(
        documentId,
        DocumentStatus.PROCESSING,
        {
          processingStartedAt: new Date(),
        },
      );

      // Audit log
      this.auditService.logAuthEvent({
        userId: document.userId,
        provider: 'document-processing',
        event: DocumentEventType.DOCUMENT_PROCESSING_STARTED as any,
        success: true,
        metadata: { documentId },
      });

      // INTELLIGENT PDF ROUTING
      let ocrResult: any;
      let processingMethod: ProcessingMethod;

      this.logger.log(
        `[PDF PROCESSING] Starting processing for document ${documentId}`,
      );
      this.logger.log(
        `[PDF PROCESSING] MimeType: ${mimeType}, Has buffer: ${!!fileBuffer}`,
      );

      if (mimeType === 'application/pdf' && fileBuffer) {
        // Try pdf2json extraction first
        try {
          this.logger.log(
            `[PDF2JSON] Starting pdf2json extraction for document ${documentId}...`,
          );
          this.logger.debug(
            `[PDF2JSON] Buffer size: ${fileBuffer.length} bytes, MimeType: ${mimeType}`,
          );

          const { chunks, meta } =
            await this.pdf2JsonService.parseBuffer(fileBuffer);

          this.logger.log(
            `[PDF2JSON] Extraction complete: ${chunks.length} chunks from ${meta.Pages?.length} pages`,
          );

          // Log sample chunk for debugging
          if (chunks.length > 0) {
            this.logger.log(
              `[PDF2JSON] Chunk sample: ${chunks[0]?.content.substring(0, 200)}`,
            );
          }

          // Combine all chunk content for entity extraction
          const fullText = chunks.map((c) => c.content).join('\n');
          this.logger.log(`[PDF2JSON] Full text length: ${fullText.length}`);

          // Check if we got meaningful text
          if (fullText.trim().length < 50) {
            this.logger.warn(
              `[PDF2JSON] Insufficient text extracted (${fullText.length} chars), falling back to OCR`,
            );
            throw new Error(
              `Insufficient text content: ${fullText.length} characters`,
            );
          }

          // Extract entities from combined text using regex patterns
          const entities = extractEntitiesFromText(fullText);
          this.logger.log(
            `[PDF2JSON] Extracted ${entities.length} entities from text`,
          );

          ocrResult = {
            text: fullText,
            confidence: 1.0, // Native text = 100% confidence
            pageCount: meta.Pages?.length || chunks.length,
            entities, // Use regex-based entity extraction
            fullResponse: {
              method: 'pdf2json_extraction',
              chunks, // Include structured chunks
              metadata: meta.Meta || {},
            },
          };

          processingMethod = ProcessingMethod.DIRECT_EXTRACTION;
        } catch (pdf2jsonError) {
          // pdf2json failed - check error type for intelligent fallback
          const errorMessage = pdf2jsonError.message || String(pdf2jsonError);
          const isXRefError = errorMessage.includes(
            'Invalid XRef stream header',
          );

          this.logger.warn(`[PDF2JSON] pdf2json failed for ${documentId}`);
          this.logger.warn(`[PDF2JSON] Error details: ${errorMessage}`);
          this.logger.debug(
            `[PDF2JSON] Error stack: ${pdf2jsonError.stack || 'No stack trace'}`,
          );

          // Multi-tier fallback strategy:
          // 1. If XRef error, try pdf-parse (handles corrupted XRef better)
          // 2. If pdf-parse fails or other error, use OCR
          if (isXRefError) {
            this.logger.log(
              `[PDF-PARSE] pdf2json detected XRef error - trying pdf-parse as intermediate fallback...`,
            );

            try {
              // Try pdf-parse as fallback (more resilient to XRef issues)
              const pdfData = await pdfParse(fileBuffer);
              const extractedText = pdfData.text || '';

              this.logger.log(
                `[PDF-PARSE] pdf-parse extraction successful: ${extractedText.length} characters`,
              );

              // Check if we got meaningful text
              if (extractedText.trim().length >= 50) {
                // Success with pdf-parse!
                const entities = extractEntitiesFromText(extractedText);
                this.logger.log(
                  `[PDF-PARSE] Extracted ${entities.length} entities from text`,
                );

                ocrResult = {
                  text: extractedText,
                  confidence: 1.0, // Native text = 100% confidence
                  pageCount: pdfData.numpages || 1,
                  entities,
                  fullResponse: {
                    method: 'pdf_parse_extraction',
                    metadata: {
                      numPages: pdfData.numpages,
                      info: pdfData.info,
                      reason: 'pdf2json_xref_error_fallback',
                    },
                  },
                };

                processingMethod = ProcessingMethod.DIRECT_EXTRACTION;

                // Log for tracking problematic PDFs
                this.logger.log(
                  `[PDF-PARSE] Successfully recovered from XRef error for document ${documentId}`,
                );
              } else {
                // Not enough text even with pdf-parse - fall back to OCR
                throw new Error(
                  `Insufficient text from pdf-parse: ${extractedText.length} characters`,
                );
              }
            } catch (pdfParseError) {
              // pdf-parse also failed - fall back to OCR
              this.logger.warn(
                `[PDF-PARSE] pdf-parse also failed for ${documentId}, falling back to OCR`,
              );
              this.logger.warn(
                `[PDF-PARSE] Error: ${pdfParseError.message || pdfParseError}`,
              );

              ocrResult = await this.ocrService.processDocument(
                gcsUri,
                mimeType,
                document.pageCount,
              );
              this.logger.log(
                `[PDF PROCESSING] OCR fallback completed. Result has entities: ${!!ocrResult.entities}, count: ${ocrResult.entities?.length || 0}`,
              );
              processingMethod =
                document.pageCount && document.pageCount <= 15
                  ? ProcessingMethod.OCR_SYNC
                  : ProcessingMethod.OCR_BATCH;
            }
          } else {
            // Not an XRef error - go directly to OCR
            this.logger.log(
              `[PDF PROCESSING] Non-XRef error, falling back directly to OCR`,
            );

            ocrResult = await this.ocrService.processDocument(
              gcsUri,
              mimeType,
              document.pageCount,
            );
            this.logger.log(
              `[PDF PROCESSING] OCR fallback completed. Result has entities: ${!!ocrResult.entities}, count: ${ocrResult.entities?.length || 0}`,
            );
            processingMethod =
              document.pageCount && document.pageCount <= 15
                ? ProcessingMethod.OCR_SYNC
                : ProcessingMethod.OCR_BATCH;
          }
        }
      } else {
        // Not a PDF or no buffer available - use standard OCR
        this.logger.log(
          `[PDF PROCESSING] Not a PDF or no buffer - using standard OCR`,
        );
        ocrResult = await this.ocrService.processDocument(
          gcsUri,
          mimeType,
          document.pageCount,
        );
        this.logger.log(
          `[PDF PROCESSING] OCR completed. Result has entities: ${!!ocrResult.entities}, count: ${ocrResult.entities?.length || 0}`,
        );
        processingMethod = mimeType.startsWith('image/')
          ? ProcessingMethod.OCR_SYNC
          : ProcessingMethod.OCR_BATCH;
      }

      this.logger.log(
        `[PDF PROCESSING] Processing method determined: ${processingMethod}`,
      );

      // Store processed output JSON
      const processedUri = await this.storageService.storeProcessed(
        ocrResult.fullResponse,
        {
          documentId,
          userId: document.userId,
          fileName: `${documentId}.json`,
          mimeType: 'application/json',
          contentLength: JSON.stringify(ocrResult.fullResponse).length,
        },
      );

      // Extract and save structured fields
      await this.extractAndSaveFields(documentId, ocrResult);

      // Update document with results
      // Validate state transition before updating
      const currentDocument = await this.documentRepository.findById(documentId);
      if (currentDocument) {
        DocumentStateMachine.validateTransition(
          currentDocument.status,
          DocumentStatus.PROCESSED,
        );
      }

      await this.documentRepository.updateStatus(
        documentId,
        DocumentStatus.PROCESSED,
        {
          processedFileUri: processedUri,
          ocrJsonOutput: ocrResult.fullResponse,
          extractedText: ocrResult.text,
          confidence: ocrResult.confidence,
          pageCount: ocrResult.pageCount,
          processedAt: new Date(),
          processingMethod,
        },
      );

      // Audit log
      this.auditService.logAuthEvent({
        userId: document.userId,
        provider: 'document-processing',
        event: DocumentEventType.DOCUMENT_PROCESSING_COMPLETED as any,
        success: true,
        metadata: {
          documentId,
          confidence: ocrResult.confidence,
          entitiesCount: ocrResult.entities?.length || 0,
        },
      });

      this.logger.log(`Processing complete for document ${documentId}`);
    } catch (error) {
      await this.handleProcessingError(documentId, error);
    }
  }

  /**
   * Extract structured fields from OCR result
   */
  private async extractAndSaveFields(
    documentId: string,
    ocrResult: any,
  ): Promise<void> {
    this.logger.log(
      `[FIELD EXTRACTION] Starting field extraction for document ${documentId}`,
    );
    this.logger.log(
      `[FIELD EXTRACTION] OCR result structure: ${JSON.stringify({
        hasEntities: !!ocrResult.entities,
        entitiesCount: ocrResult.entities?.length || 0,
        hasFullResponse: !!ocrResult.fullResponse,
        keys: Object.keys(ocrResult),
      })}`,
    );

    if (!ocrResult.entities || ocrResult.entities.length === 0) {
      this.logger.warn(
        `[FIELD EXTRACTION] No entities found in OCR result for document ${documentId}`,
      );
      return;
    }

    const fields: ExtractedField[] = [];
    let lowConfidenceCount = 0;

    for (const entity of ocrResult.entities) {
      this.logger.debug(
        `[FIELD EXTRACTION] Processing entity: ${JSON.stringify({
          type: entity.type,
          mentionText: entity.mentionText?.substring(0, 50),
          confidence: entity.confidence,
        })}`,
      );

      // CHANGED: Save ALL entities, including low-confidence ones
      // This gives users visibility into all data the model extracted
      const field = new ExtractedField();
      // NOTE: ID will be auto-generated by database as UUID
      field.documentId = documentId;
      field.fieldKey = this.normalizeEntityType(entity.type);
      field.fieldValue = entity.mentionText;
      field.fieldType = this.mapToFieldType(entity.type);
      field.confidence = entity.confidence;
      field.startIndex = entity.startOffset;
      field.endIndex = entity.endOffset;
      field.createdAt = new Date();
      field.updatedAt = new Date();

      fields.push(field);

      // Track low-confidence fields for logging
      if (entity.confidence < 0.7) {
        lowConfidenceCount++;
      }

      this.logger.debug(
        `[FIELD EXTRACTION] Added field: ${field.fieldKey} = ${field.fieldValue?.substring(0, 50)} (confidence: ${field.confidence})`,
      );
    }

    this.logger.log(
      `[FIELD EXTRACTION] Extraction complete: ${fields.length} fields to save (${lowConfidenceCount} low-confidence included)`,
    );

    if (fields.length > 0) {
      this.logger.log(
        `[FIELD EXTRACTION] Saving ${fields.length} fields to database...`,
      );
      await this.documentRepository.saveExtractedFields(fields);
      this.logger.log(
        `[FIELD EXTRACTION] Successfully saved ${fields.length} extracted fields for document ${documentId}`,
      );
    } else {
      this.logger.warn(
        `[FIELD EXTRACTION] No fields to save for document ${documentId} (all filtered out or none extracted)`,
      );
    }
  }

  /**
   * Handle processing errors
   */
  private async handleProcessingError(
    documentId: string,
    error: any,
  ): Promise<void> {
    const document = await this.documentRepository.findById(documentId);
    if (!document) return;

    const retryCount = (document.retryCount || 0) + 1;
    const errorMessage = this.sanitizeError(error);

    if (retryCount < this.maxRetryCount) {
      // Retry
      this.logger.warn(
        `Processing failed for document ${documentId}, retry ${retryCount}/${this.maxRetryCount}`,
      );

      // Validate state transition for retry
      DocumentStateMachine.validateTransition(
        document.status,
        DocumentStatus.QUEUED,
      );

      await this.documentRepository.update(documentId, {
        retryCount,
        errorMessage,
        status: DocumentStatus.QUEUED,
      });

      // Retry after delay (note: no buffer available on retry, will use OCR)
      setTimeout(() => {
        this.startProcessing(
          documentId,
          document.rawFileUri,
          document.mimeType,
        ).catch(() => {});
      }, 30000 * retryCount); // Exponential backoff: 30s, 60s, 90s
    } else {
      // Validate state transition before marking as failed
      DocumentStateMachine.validateTransition(
        document.status,
        DocumentStatus.FAILED,
      );

      // Mark as failed
      await this.documentRepository.updateStatus(
        documentId,
        DocumentStatus.FAILED,
        {
          errorMessage,
          retryCount,
        },
      );

      // Audit log
      this.auditService.logAuthEvent({
        userId: document.userId,
        provider: 'document-processing',
        event: DocumentEventType.DOCUMENT_PROCESSING_FAILED as any,
        success: false,
        errorMessage,
        metadata: { documentId, retryCount },
      });

      this.logger.error(
        `Processing permanently failed for document ${documentId}`,
      );
    }
  }

  /**
   * Get document by ID (with authorization check)
   */
  async getDocument(
    documentId: string,
    userId: string | number,
  ): Promise<Document> {
    // Debug: Check if document exists at all
    const documentExists = await this.documentRepository.findById(documentId);

    this.logger.debug(
      `[AUTH] Checking access: documentId=${documentId}, requestUserId=${userId}, documentExists=${!!documentExists}, documentOwnerId=${documentExists?.userId}`,
    );

    const document = await this.documentRepository.findByIdAndUserId(
      documentId,
      userId,
    );

    if (!document) {
      // Log detailed info for debugging
      if (documentExists) {
        this.logger.warn(
          `[AUTH] Document ${documentId} exists but belongs to user ${documentExists.userId}, not ${userId}`,
        );
      } else {
        this.logger.warn(
          `[AUTH] Document ${documentId} does not exist in database`,
        );
      }

      // Audit log unauthorized access attempt
      this.auditService.logAuthEvent({
        userId,
        provider: 'document-processing',
        event: DocumentEventType.UNAUTHORIZED_DOCUMENT_ACCESS as any,
        success: false,
        metadata: {
          documentId,
          documentExists: !!documentExists,
          documentOwnerId: documentExists?.userId,
        },
      });

      throw new NotFoundException('Document not found');
    }

    // Audit log access
    this.auditService.logAuthEvent({
      userId,
      provider: 'document-processing',
      event: DocumentEventType.DOCUMENT_ACCESSED as any,
      success: true,
      metadata: { documentId },
    });

    return document;
  }

  /**
   * List user's documents
   */
  async listDocuments(
    userId: string | number,
    options?: { page?: number; limit?: number; status?: DocumentStatus[] },
  ): Promise<{ data: Document[]; total: number; page: number; limit: number }> {
    const page = options?.page || 1;
    const limit = options?.limit || 20;
    const skip = (page - 1) * limit;

    const result = await this.documentRepository.findByUserId(userId, {
      skip,
      limit,
      status: options?.status,
    });

    return {
      ...result,
      page,
      limit,
    };
  }

  /**
   * Soft delete document
   */
  async deleteDocument(
    documentId: string,
    userId: string | number,
  ): Promise<void> {
    const document = await this.getDocument(documentId, userId); // Checks authorization

    if (document.deletedAt) {
      throw new ForbiddenException('Document already deleted');
    }

    const scheduledDeletionAt = new Date();
    scheduledDeletionAt.setFullYear(
      scheduledDeletionAt.getFullYear() + this.retentionYears,
    );

    await this.documentRepository.update(documentId, {
      deletedAt: new Date(),
      scheduledDeletionAt,
    });

    // Audit log
    this.auditService.logAuthEvent({
      userId,
      provider: 'document-processing',
      event: DocumentEventType.DOCUMENT_DELETED as any,
      success: true,
      metadata: {
        documentId,
        scheduledDeletionAt: scheduledDeletionAt.toISOString(),
      },
    });

    this.logger.log(
      `Document soft-deleted: ${documentId} (hard delete at ${scheduledDeletionAt.toISOString()})`,
    );
  }

  /**
   * Trigger OCR processing (origin manager only)
   * 
   * Authority Rules (from Phase 2):
   * - Only origin manager can trigger OCR
   * - Document must be in STORED, PROCESSED, or FAILED state
   * - Re-processing allowed (PROCESSED → PROCESSING)
   * 
   * @param documentId - Document UUID
   * @param actor - Actor requesting OCR trigger (must be origin manager)
   * @throws ForbiddenException if actor is not origin manager
   * @throws BadRequestException if document state doesn't allow processing
   */
  async triggerOcr(documentId: string, actor: Actor): Promise<void> {
    // 1. Get document
    const document = await this.documentRepository.findById(documentId);
    if (!document) {
      throw new NotFoundException('Document not found');
    }

    // 2. Validate actor is origin manager
    if (
      actor.type !== 'manager' ||
      document.originManagerId !== actor.id
    ) {
      this.auditService.logAuthEvent({
        userId: String(actor.id),
        provider: 'document-processing',
        event: DocumentEventType.UNAUTHORIZED_DOCUMENT_ACCESS as any,
        success: false,
        metadata: {
          documentId,
          actorType: actor.type,
          actorId: actor.id,
          operation: 'trigger-ocr',
          originManagerId: document.originManagerId,
        },
      });

      throw new ForbiddenException(
        'Only the origin manager can trigger OCR processing',
      );
    }

    // 3. Validate document state allows processing
    if (!DocumentStateMachine.canProcess(document.status)) {
      throw new BadRequestException(
        `Document cannot be processed in current state: ${document.status}. ` +
          `Document must be in STORED, PROCESSED, or FAILED state to trigger OCR.`,
      );
    }

    // 4. Validate state transition
    DocumentStateMachine.validateTransition(
      document.status,
      DocumentStatus.PROCESSING,
    );

    // 5. Trigger processing (async, don't await)
    this.startProcessing(
      documentId,
      document.rawFileUri,
      document.mimeType,
    ).catch((error) => {
      this.logger.error(
        `Failed to trigger OCR for document ${documentId}: ${error.message}`,
      );
    });

    // 6. Audit log
    this.auditService.logAuthEvent({
      userId: String(actor.id),
      provider: 'document-processing',
      event: DocumentEventType.DOCUMENT_PROCESSING_STARTED as any,
      success: true,
      metadata: {
        documentId,
        actorType: actor.type,
        actorId: actor.id,
        operation: 'trigger-ocr',
        fromStatus: document.status,
      },
    });

    this.logger.log(
      `OCR triggered for document ${documentId} by origin manager ${actor.id}`,
    );
  }

  /**
   * Generate signed download URL
   */
  async getDownloadUrl(
    documentId: string,
    userId: string | number,
  ): Promise<string> {
    const document = await this.getDocument(documentId, userId); // Checks authorization

    if (!document.rawFileUri) {
      throw new NotFoundException('Document file not available');
    }

    return this.storageService.getSignedUrl(document.rawFileUri, 86400); // 24 hours
  }

  /**
   * Get extracted fields for document
   */
  async getExtractedFields(
    documentId: string,
    userId: string | number,
  ): Promise<ExtractedField[]> {
    this.logger.log(
      `[FIELD RETRIEVAL] Getting extracted fields for document ${documentId}`,
    );
    await this.getDocument(documentId, userId); // Authorization check
    const fields =
      await this.documentRepository.findExtractedFieldsByDocumentId(documentId);
    this.logger.log(
      `[FIELD RETRIEVAL] Found ${fields.length} fields for document ${documentId}`,
    );
    if (fields.length > 0) {
      this.logger.debug(
        `[FIELD RETRIEVAL] Sample field: ${JSON.stringify({
          fieldKey: fields[0].fieldKey,
          fieldValue: fields[0].fieldValue?.substring(0, 50),
          fieldType: fields[0].fieldType,
          confidence: fields[0].confidence,
        })}`,
      );
    }
    return fields;
  }

  /**
   * Scheduled job: Hard delete documents after retention period
   * Runs daily at 3:00 AM UTC
   *
   * HIPAA Compliance:
   * - Deletes both GCS files and database records
   * - Logs all deletions for audit trail
   * - Retains audit logs even after document deletion
   */
  @Cron(CronExpression.EVERY_DAY_AT_3AM)
  async cleanupExpiredDocuments(): Promise<void> {
    this.logger.log('Starting scheduled document cleanup job...');

    try {
      const expiredDocuments = await this.documentRepository.findExpired();

      if (expiredDocuments.length === 0) {
        this.logger.log('No expired documents to clean up');
        return;
      }

      this.logger.log(
        `Found ${expiredDocuments.length} expired documents to delete`,
      );

      let successCount = 0;
      let failureCount = 0;

      for (const document of expiredDocuments) {
        try {
          // 1. Delete raw file from GCS
          if (document.rawFileUri) {
            await this.storageService.delete(document.rawFileUri);
          }

          // 2. Delete processed file from GCS (if exists)
          if (document.processedFileUri) {
            await this.storageService.delete(document.processedFileUri);
          }

          // 3. Hard delete from database (cascade deletes extracted_fields)
          await this.documentRepository.hardDelete(document.id);

          // 4. Audit log (CRITICAL: Log persists even after document deletion)
          this.auditService.logAuthEvent({
            userId: document.userId,
            provider: 'document-processing',
            event: DocumentEventType.DOCUMENT_HARD_DELETED as any,
            success: true,
            metadata: {
              documentId: document.id,
              documentType: document.documentType,
              deletedAt: document.deletedAt?.toISOString(),
              retentionYears: this.retentionYears,
              reason: 'scheduled_cleanup',
            },
          });

          successCount++;
          this.logger.debug(`Hard deleted document ${document.id}`);
        } catch (error) {
          failureCount++;
          this.logger.error(
            `Failed to hard delete document ${document.id}: ${error.message}`,
          );

          // Log failure but continue with other documents
          this.auditService.logAuthEvent({
            userId: document.userId,
            provider: 'document-processing',
            event: DocumentEventType.DOCUMENT_HARD_DELETED as any,
            success: false,
            errorMessage: this.sanitizeError(error),
            metadata: {
              documentId: document.id,
              reason: 'scheduled_cleanup_failed',
            },
          });
        }
      }

      this.logger.log(
        `Cleanup job complete: ${successCount} deleted, ${failureCount} failed`,
      );
    } catch (error) {
      this.logger.error(`Cleanup job failed: ${error.message}`);
    }
  }

  // Helper methods

  private normalizeEntityType(type: string): string {
    // Map Document AI entity types to our field keys
    const mapping: Record<string, string> = {
      person_name: 'patient_name',
      date: 'test_date',
      number: 'result_value',
      // TODO: Add more mappings based on processor configuration
    };

    return mapping[type] || type;
  }

  private mapToFieldType(entityType: string): string {
    if (entityType.includes('date')) return 'date';
    if (entityType.includes('number') || entityType.includes('quantity'))
      return 'number';
    return 'string';
  }

  private sanitizeError(error: any): string {
    const message = error?.message || String(error);
    return message
      .replace(/gs:\/\/[^\s]+/g, '[URI_REDACTED]')
      .replace(/projects\/[^\/\s]+/g, '[PROJECT_REDACTED]')
      .substring(0, 500);
  }
}
